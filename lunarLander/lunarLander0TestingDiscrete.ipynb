{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for testing the pretrained weights for Lunar Lander Discrete Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the model and loads the desired weights (they need to be in the same directory than the notebook)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add (keras.layers.Dense(units = 128, input_shape=(8,), activation = \"relu\"))\n",
    "model.add (keras.layers.Dense(units = 128, activation = \"relu\"))\n",
    "model.add (keras.layers.Dense(units = 32, activation = \"relu\"))\n",
    "model.add (keras.layers.Dense(units = 4))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss =\"mse\")\n",
    "#print (model.summary())\n",
    "model.load_weights(\"lunarLanderDiscrete1500episodes.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop (Google colab cannot render the environment, only computations can be done. Render in Local Machine!)\n",
    "\n",
    "numberOfTestingEpisodes = 5\n",
    "testingRewards = []\n",
    "for episode in range (numberOfTestingEpisodes):\n",
    "    done = False\n",
    "    episodeReward = 0\n",
    "    prevObs = env.reset()\n",
    "    for step in range (1010):\n",
    "       \n",
    "        if done:\n",
    "            break\n",
    "        #env.render()\n",
    "        \n",
    "       \n",
    "\n",
    "        target = model.predict(np.expand_dims(prevObs, axis = 0))[0]\n",
    "\n",
    "        action = np.argmax(target)\n",
    "        \n",
    "        \n",
    "        obs, reward, done, info = env.step (action)\n",
    "        \n",
    "        prevObs = obs\n",
    "        episodeReward += reward\n",
    "      \n",
    "    print (episode, \" Episode Reward: \", episodeReward)\n",
    "    testingRewards.append(episodeReward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot (list(range(len(testingRewards))), testingRewards)\n",
    "plt.plot (list(range(len(testingRewards))), [np.average(testingRewards)]*len(testingRewards))\n",
    "plt.title(\"Rewards obtained during Testing\")\n",
    "plt.xlabel(\"Episode number\")\n",
    "plt.ylabel(\"Reward obtained\")\n",
    "plt.legend([\"Rewards\", \"Average: %.2f\" % np.average(testingRewards)])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
