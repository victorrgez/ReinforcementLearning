{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RNRqsLfU2r-"
   },
   "source": [
    "## Solving Pendulum Environment with Gaussian Noise and Actor-Critic\n",
    "### This noise technique is commonly used instead of epsilon-greedy exploration for continuous action-spaces.\n",
    "### The exploration movements (actions with noise) will be saved in the agent's memory in order to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62pEsGHa-V8a"
   },
   "outputs": [],
   "source": [
    "# Imports the libraries:\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8GdixeE-cci"
   },
   "outputs": [],
   "source": [
    "stateInput        = keras.Input(shape = (3,))    \n",
    "\n",
    "actorDense1       = keras.layers.Dense(units = 128, activation = \"relu\")   (stateInput)\n",
    "actorDense2       = keras.layers.Dense(units = 128, activation = \"relu\")   (actorDense1)\n",
    "actorDense3       = keras.layers.Dense(units = 32, activation = \"relu\")    (actorDense2)\n",
    "actorOutput       = keras.layers.Dense(units = 1, activation = \"tanh\")     (actorDense3)\n",
    "\n",
    "actorModel        = keras.Model (inputs = stateInput, outputs = actorOutput)\n",
    "actorModel.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss =\"mse\")\n",
    "\n",
    "#print (actorModel.summary())\n",
    "\n",
    "# Creating the critic:\n",
    "\n",
    "stateDense1       = keras.layers.Dense(units = 128, activation=\"relu\")      (stateInput)\n",
    "stateDense2       = keras.layers.Dense(units = 64, activation=\"relu\")       (stateDense1)\n",
    "actionInput       = keras.Input(shape = (1,), name=\"actionInput\")                            \n",
    "actionDense1      = keras.layers.Dense(units = 64, activation=\"relu\")       (actionInput)       \n",
    "\n",
    "criticConcatenate = keras.layers.Concatenate()                              ([stateDense2, actionDense1])\n",
    "criticDense1      = keras.layers.Dense(units = 128, activation = \"relu\")    (criticConcatenate)\n",
    "criticDense2      = keras.layers.Dense(units = 128, activation = \"relu\")    (criticDense1)\n",
    "criticOutput      = keras.layers.Dense(units = 1)                           (criticDense2)\n",
    "\n",
    "criticModel = keras.Model(inputs = [stateInput, actionInput] , outputs = criticOutput )\n",
    "criticModel.compile(optimizer = keras.optimizers.Adam(lr=0.001), loss = \"mse\")\n",
    "\n",
    "\n",
    "# Creating the target networks for both the actor and the critic:\n",
    "\n",
    "staticActorModel  = keras.models.clone_model( actorModel, input_tensors=None, clone_function=None)\n",
    "staticCriticModel = keras.models.clone_model( criticModel, input_tensors=None, clone_function=None)\n",
    "\n",
    "\n",
    "\n",
    "#print (criticModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOTZHzwj-eXb"
   },
   "outputs": [],
   "source": [
    "def updateStaticWeights(model, staticModel,tau):\n",
    "    modelWeights = model.get_weights()\n",
    "    staticWeights = staticModel.get_weights()\n",
    "    \n",
    "    #Python broadcasting:\n",
    "    \n",
    "    for i in range (len(staticWeights)):\n",
    "     \n",
    "        staticWeights[i] = (1-tau) * staticWeights[i] + tau * modelWeights[i]\n",
    "\n",
    "        \n",
    "    staticModel.set_weights(staticWeights)\n",
    "\n",
    "\n",
    "    return model, staticModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-kG7P1f-gyd"
   },
   "outputs": [],
   "source": [
    "# Memory Replay is carried out in this function.\n",
    "# 256 random previous experiences are \"remembered\" and processed in each step with a batch size of 32\n",
    "# This way we can prevent overfitting most recent states:\n",
    "\n",
    "def executeMemoryReplayStep(memoryPreviousStates, memoryStates, memoryActions, memoryRD, actorModel, staticActorModel, criticModel, staticCriticModel, itemsInMemory):\n",
    "    randomList = np.random.permutation(itemsInMemory) # After that number everything is 0\n",
    "    \n",
    "    memoryPreviousStates = memoryPreviousStates[randomList[:numberOfLearningFromReplaySteps]]\n",
    "    memoryStates = memoryStates[randomList[:numberOfLearningFromReplaySteps]]\n",
    "    memoryRD = memoryRD[randomList[:numberOfLearningFromReplaySteps]]\n",
    "    memoryActions = memoryActions[randomList[:numberOfLearningFromReplaySteps]]\n",
    "\n",
    "    \n",
    "    previousState = memoryPreviousStates\n",
    "    state = memoryStates\n",
    "    actions = memoryActions\n",
    "    reward, done = np.split(memoryRD,2,axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Critic Model Training:\n",
    "    \n",
    "    futureActions       = staticActorModel.predict (state)\n",
    "\n",
    "    valueState          = staticCriticModel.predict ([state, futureActions])\n",
    "\n",
    "    valuePreviousState = reward + discount * valueState * (1-done) #for those rows in which done is False:\n",
    "\n",
    "    criticModel.fit ( x = [previousState, actions], y = valuePreviousState, epochs=1, verbose = 0, batch_size =32)\n",
    "\n",
    "\n",
    "    \n",
    "    # Actor Model Training:    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actorModel.trainable_weights)\n",
    "        \n",
    "        predictedActions = actorModel(previousState, training=False)\n",
    " \n",
    "        criticPreds = criticModel ([previousState, predictedActions],  training=False)\n",
    "        \n",
    "    fromCriticToActorGrads = tape.gradient(criticPreds, actorModel.trainable_weights)\n",
    "    fromCriticToActorGrads = -np.array(fromCriticToActorGrads) # We are trying to do gradient ascent\n",
    "    \n",
    "    gradsAndVariables = zip (fromCriticToActorGrads, actorModel.trainable_weights)\n",
    "    actorModel.optimizer.apply_gradients(gradsAndVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CDkCjiX8-jK2",
    "outputId": "eaa87d5b-4579-44e0-e5d0-bfb6ebb6cc25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATIC WEIGHTS UPDATED\n",
      "0  Episode Reward:  -1385.4453361701587 .Sigma: 0.996666666666667\n",
      "1  Episode Reward:  -1387.2541210256888 .Sigma: 0.9933333333333341\n",
      "2  Episode Reward:  -1248.1811118897745 .Sigma: 0.9900000000000011\n",
      "3  Episode Reward:  -1169.9658895598948 .Sigma: 0.9866666666666681\n",
      "4  Episode Reward:  -1196.440277397148 .Sigma: 0.9833333333333352\n",
      "5  Episode Reward:  -1184.5153221340636 .Sigma: 0.9800000000000022\n",
      "6  Episode Reward:  -1101.816664499706 .Sigma: 0.9766666666666692\n",
      "7  Episode Reward:  -1669.745458472823 .Sigma: 0.9733333333333363\n",
      "8  Episode Reward:  -1375.2015396450988 .Sigma: 0.9700000000000033\n",
      "9  Episode Reward:  -1392.7651946302487 .Sigma: 0.9666666666666703\n",
      "10  Episode Reward:  -1206.2331595851938 .Sigma: 0.9633333333333374\n",
      "11  Episode Reward:  -1085.8981299445477 .Sigma: 0.9600000000000044\n",
      "12  Episode Reward:  -1159.59624709566 .Sigma: 0.9566666666666714\n",
      "13  Episode Reward:  -1215.5835793314427 .Sigma: 0.9533333333333385\n",
      "14  Episode Reward:  -1062.4197744666355 .Sigma: 0.9500000000000055\n",
      "STATIC WEIGHTS UPDATED\n",
      "15  Episode Reward:  -1179.8057172515048 .Sigma: 0.9466666666666725\n",
      "16  Episode Reward:  -1373.4710412294362 .Sigma: 0.9433333333333396\n",
      "17  Episode Reward:  -1295.1007204550772 .Sigma: 0.9400000000000066\n",
      "18  Episode Reward:  -1748.0348692727525 .Sigma: 0.9366666666666736\n",
      "19  Episode Reward:  -1439.8118143265892 .Sigma: 0.9333333333333407\n",
      "20  Episode Reward:  -1519.3350680233248 .Sigma: 0.9300000000000077\n",
      "21  Episode Reward:  -1449.3621403847837 .Sigma: 0.9266666666666747\n",
      "22  Episode Reward:  -1437.9137832251447 .Sigma: 0.9233333333333418\n",
      "23  Episode Reward:  -1295.0976892775395 .Sigma: 0.9200000000000088\n",
      "24  Episode Reward:  -1470.1242749426633 .Sigma: 0.9166666666666758\n",
      "25  Episode Reward:  -1604.025369382656 .Sigma: 0.9133333333333429\n",
      "26  Episode Reward:  -1651.8127332736397 .Sigma: 0.9100000000000099\n",
      "27  Episode Reward:  -1306.8903029224898 .Sigma: 0.906666666666677\n",
      "28  Episode Reward:  -1447.7000294343825 .Sigma: 0.903333333333344\n",
      "29  Episode Reward:  -1692.4976923871084 .Sigma: 0.900000000000011\n",
      "STATIC WEIGHTS UPDATED\n",
      "30  Episode Reward:  -1667.7703558952073 .Sigma: 0.896666666666678\n",
      "31  Episode Reward:  -1457.1255090358732 .Sigma: 0.8933333333333451\n",
      "32  Episode Reward:  -1672.5614074787634 .Sigma: 0.8900000000000121\n",
      "33  Episode Reward:  -1689.7923974876048 .Sigma: 0.8866666666666791\n",
      "34  Episode Reward:  -1500.4295592779135 .Sigma: 0.8833333333333462\n",
      "35  Episode Reward:  -1747.380853361725 .Sigma: 0.8800000000000132\n",
      "36  Episode Reward:  -1723.5745100112683 .Sigma: 0.8766666666666802\n",
      "37  Episode Reward:  -1715.9109092572662 .Sigma: 0.8733333333333473\n",
      "38  Episode Reward:  -1386.3126782028182 .Sigma: 0.8700000000000143\n",
      "39  Episode Reward:  -1709.8016233773717 .Sigma: 0.8666666666666814\n",
      "40  Episode Reward:  -1647.7645899840077 .Sigma: 0.8633333333333484\n",
      "41  Episode Reward:  -1698.8476899727132 .Sigma: 0.8600000000000154\n",
      "42  Episode Reward:  -1718.9270316809539 .Sigma: 0.8566666666666825\n",
      "43  Episode Reward:  -1730.8306665326063 .Sigma: 0.8533333333333495\n",
      "44  Episode Reward:  -1692.7110373437001 .Sigma: 0.8500000000000165\n",
      "STATIC WEIGHTS UPDATED\n",
      "45  Episode Reward:  -1618.4982863656605 .Sigma: 0.8466666666666836\n",
      "46  Episode Reward:  -1536.7239849479688 .Sigma: 0.8433333333333506\n",
      "47  Episode Reward:  -1620.227299625742 .Sigma: 0.8400000000000176\n",
      "48  Episode Reward:  -1706.2318242868428 .Sigma: 0.8366666666666847\n",
      "49  Episode Reward:  -1706.9403672697617 .Sigma: 0.8333333333333517\n",
      "50  Episode Reward:  -1739.3016986373173 .Sigma: 0.8300000000000187\n",
      "51  Episode Reward:  -1516.3980684133517 .Sigma: 0.8266666666666858\n",
      "52  Episode Reward:  -1742.8459658486663 .Sigma: 0.8233333333333528\n",
      "53  Episode Reward:  -1744.9524469282399 .Sigma: 0.8200000000000198\n",
      "54  Episode Reward:  -1657.1414417210942 .Sigma: 0.8166666666666869\n",
      "55  Episode Reward:  -1453.5995917744215 .Sigma: 0.8133333333333539\n",
      "56  Episode Reward:  -1487.1486775718204 .Sigma: 0.8100000000000209\n",
      "57  Episode Reward:  -1670.8386081502929 .Sigma: 0.806666666666688\n",
      "58  Episode Reward:  -1699.3551013929562 .Sigma: 0.803333333333355\n",
      "59  Episode Reward:  -1692.2287781035163 .Sigma: 0.800000000000022\n",
      "STATIC WEIGHTS UPDATED\n",
      "60  Episode Reward:  -1578.089756571779 .Sigma: 0.7966666666666891\n",
      "61  Episode Reward:  -1696.6965391156966 .Sigma: 0.7933333333333561\n",
      "62  Episode Reward:  -1674.1590184907316 .Sigma: 0.7900000000000231\n",
      "63  Episode Reward:  -1605.0426814136192 .Sigma: 0.7866666666666902\n",
      "64  Episode Reward:  -1641.0288244690546 .Sigma: 0.7833333333333572\n",
      "65  Episode Reward:  -1653.9530756996674 .Sigma: 0.7800000000000242\n",
      "66  Episode Reward:  -1486.9371699123712 .Sigma: 0.7766666666666913\n",
      "67  Episode Reward:  -1535.760670085278 .Sigma: 0.7733333333333583\n",
      "68  Episode Reward:  -1456.4970506474342 .Sigma: 0.7700000000000253\n",
      "69  Episode Reward:  -1520.922096379491 .Sigma: 0.7666666666666924\n",
      "70  Episode Reward:  -1727.3520075702545 .Sigma: 0.7633333333333594\n",
      "71  Episode Reward:  -1610.5926073659696 .Sigma: 0.7600000000000264\n",
      "72  Episode Reward:  -1682.887402043002 .Sigma: 0.7566666666666935\n",
      "73  Episode Reward:  -1480.7869921565796 .Sigma: 0.7533333333333605\n",
      "74  Episode Reward:  -1591.9992333304087 .Sigma: 0.7500000000000275\n",
      "STATIC WEIGHTS UPDATED\n",
      "75  Episode Reward:  -1562.248604169572 .Sigma: 0.7466666666666946\n",
      "76  Episode Reward:  -1422.2368035650304 .Sigma: 0.7433333333333616\n",
      "77  Episode Reward:  -1710.9355521348002 .Sigma: 0.7400000000000286\n",
      "78  Episode Reward:  -1591.604434608508 .Sigma: 0.7366666666666957\n",
      "79  Episode Reward:  -1585.1329587707114 .Sigma: 0.7333333333333627\n",
      "80  Episode Reward:  -1561.062779406706 .Sigma: 0.7300000000000297\n",
      "81  Episode Reward:  -1598.3175989677727 .Sigma: 0.7266666666666968\n",
      "82  Episode Reward:  -1737.6401980953174 .Sigma: 0.7233333333333638\n",
      "83  Episode Reward:  -1455.4634923954486 .Sigma: 0.7200000000000308\n",
      "84  Episode Reward:  -1440.9123477932926 .Sigma: 0.7166666666666979\n",
      "85  Episode Reward:  -1524.6204476052758 .Sigma: 0.7133333333333649\n",
      "86  Episode Reward:  -1459.37409790969 .Sigma: 0.7100000000000319\n",
      "87  Episode Reward:  -1735.7585236695315 .Sigma: 0.706666666666699\n",
      "88  Episode Reward:  -1739.114394072764 .Sigma: 0.703333333333366\n",
      "89  Episode Reward:  -1427.7922860922477 .Sigma: 0.700000000000033\n",
      "STATIC WEIGHTS UPDATED\n",
      "90  Episode Reward:  -1222.047498755332 .Sigma: 0.6966666666667001\n",
      "91  Episode Reward:  -1510.8193119647806 .Sigma: 0.6933333333333671\n",
      "92  Episode Reward:  -1731.3559829758235 .Sigma: 0.6900000000000341\n",
      "93  Episode Reward:  -1210.33878798982 .Sigma: 0.6866666666667012\n",
      "94  Episode Reward:  -1559.3584008704438 .Sigma: 0.6833333333333682\n",
      "95  Episode Reward:  -1512.0830074302507 .Sigma: 0.6800000000000352\n",
      "96  Episode Reward:  -1476.3624441069567 .Sigma: 0.6766666666667023\n",
      "97  Episode Reward:  -1387.6227311261168 .Sigma: 0.6733333333333693\n",
      "98  Episode Reward:  -1308.009316411794 .Sigma: 0.6700000000000363\n",
      "99  Episode Reward:  -1534.0683754167135 .Sigma: 0.6666666666667034\n",
      "100  Episode Reward:  -1.0634368583314617 .Sigma: 0.6633333333333704\n",
      "101  Episode Reward:  -1384.6828155005146 .Sigma: 0.6600000000000374\n",
      "102  Episode Reward:  -1361.324627864706 .Sigma: 0.6566666666667045\n",
      "103  Episode Reward:  -1469.3153638149022 .Sigma: 0.6533333333333715\n",
      "104  Episode Reward:  -1320.2916994268846 .Sigma: 0.6500000000000385\n",
      "STATIC WEIGHTS UPDATED\n",
      "105  Episode Reward:  -1376.4202328906931 .Sigma: 0.6466666666667056\n",
      "106  Episode Reward:  -1428.7482447135 .Sigma: 0.6433333333333726\n",
      "107  Episode Reward:  -1614.5927913564813 .Sigma: 0.6400000000000396\n",
      "108  Episode Reward:  -1150.4789243661876 .Sigma: 0.6366666666667067\n",
      "109  Episode Reward:  -1.1748492256002019 .Sigma: 0.6333333333333737\n",
      "110  Episode Reward:  -1528.6948688815464 .Sigma: 0.6300000000000407\n",
      "111  Episode Reward:  -1485.7324875040329 .Sigma: 0.6266666666667078\n",
      "112  Episode Reward:  -1319.137167291357 .Sigma: 0.6233333333333748\n",
      "113  Episode Reward:  -1451.7873648824764 .Sigma: 0.6200000000000419\n",
      "114  Episode Reward:  -1325.858934406921 .Sigma: 0.6166666666667089\n",
      "115  Episode Reward:  -1294.1725689797865 .Sigma: 0.6133333333333759\n",
      "116  Episode Reward:  -1218.702307679846 .Sigma: 0.610000000000043\n",
      "117  Episode Reward:  -1495.7118122138245 .Sigma: 0.60666666666671\n",
      "118  Episode Reward:  -1276.693032861369 .Sigma: 0.603333333333377\n",
      "119  Episode Reward:  -1298.5999593219954 .Sigma: 0.600000000000044\n",
      "STATIC WEIGHTS UPDATED\n",
      "120  Episode Reward:  -109.15776233674654 .Sigma: 0.5966666666667111\n",
      "121  Episode Reward:  -1246.7428667334812 .Sigma: 0.5933333333333781\n",
      "122  Episode Reward:  -1205.909658021388 .Sigma: 0.5900000000000452\n",
      "123  Episode Reward:  -1488.3749432352768 .Sigma: 0.5866666666667122\n",
      "124  Episode Reward:  -1281.3584235838082 .Sigma: 0.5833333333333792\n",
      "125  Episode Reward:  -1258.2264405477022 .Sigma: 0.5800000000000463\n",
      "126  Episode Reward:  -0.6663882020878593 .Sigma: 0.5766666666667133\n",
      "127  Episode Reward:  -1203.5403556519263 .Sigma: 0.5733333333333803\n",
      "128  Episode Reward:  -1447.8136205449016 .Sigma: 0.5700000000000474\n",
      "129  Episode Reward:  -1254.6743434631107 .Sigma: 0.5666666666667144\n",
      "130  Episode Reward:  -1449.4892941971032 .Sigma: 0.5633333333333814\n",
      "131  Episode Reward:  -1324.409757491798 .Sigma: 0.5600000000000485\n",
      "132  Episode Reward:  -1250.301535946147 .Sigma: 0.5566666666667155\n",
      "133  Episode Reward:  -1270.5035345089693 .Sigma: 0.5533333333333825\n",
      "134  Episode Reward:  -1239.862170670416 .Sigma: 0.5500000000000496\n",
      "STATIC WEIGHTS UPDATED\n",
      "135  Episode Reward:  -1503.2816970323388 .Sigma: 0.5466666666667166\n",
      "136  Episode Reward:  -1326.8274583809878 .Sigma: 0.5433333333333836\n",
      "137  Episode Reward:  -1292.970165448722 .Sigma: 0.5400000000000507\n",
      "138  Episode Reward:  -1215.729439574611 .Sigma: 0.5366666666667177\n",
      "139  Episode Reward:  -1310.4365120804223 .Sigma: 0.5333333333333847\n",
      "140  Episode Reward:  -1255.5011835014059 .Sigma: 0.5300000000000518\n",
      "141  Episode Reward:  -1176.6233535512724 .Sigma: 0.5266666666667188\n",
      "142  Episode Reward:  -1143.3021389721428 .Sigma: 0.5233333333333858\n",
      "143  Episode Reward:  -1438.5483807334688 .Sigma: 0.5200000000000529\n",
      "144  Episode Reward:  -1232.136383441134 .Sigma: 0.5166666666667199\n",
      "145  Episode Reward:  -1208.9607306813473 .Sigma: 0.5133333333333869\n",
      "146  Episode Reward:  -1588.3831139209558 .Sigma: 0.510000000000054\n",
      "147  Episode Reward:  -1207.2220234043161 .Sigma: 0.506666666666721\n",
      "148  Episode Reward:  -1209.629552620357 .Sigma: 0.503333333333388\n",
      "149  Episode Reward:  -1740.1256749607724 .Sigma: 0.5000000000000551\n",
      "STATIC WEIGHTS UPDATED\n",
      "150  Episode Reward:  -1368.0748650460498 .Sigma: 0.4966666666667221\n",
      "151  Episode Reward:  -1138.1592056363615 .Sigma: 0.49333333333338913\n",
      "152  Episode Reward:  -1152.7328203879226 .Sigma: 0.49000000000005617\n",
      "153  Episode Reward:  -1141.3782068270207 .Sigma: 0.4866666666667232\n",
      "154  Episode Reward:  -1411.556013273773 .Sigma: 0.48333333333339024\n",
      "155  Episode Reward:  -1304.6530542153305 .Sigma: 0.48000000000005727\n",
      "156  Episode Reward:  -1184.4386488122675 .Sigma: 0.4766666666667243\n",
      "157  Episode Reward:  -1002.8279614319978 .Sigma: 0.47333333333339134\n",
      "158  Episode Reward:  -1123.5861305478927 .Sigma: 0.47000000000005837\n",
      "159  Episode Reward:  -1052.9741770519538 .Sigma: 0.4666666666667254\n",
      "160  Episode Reward:  -1738.4192696161851 .Sigma: 0.46333333333339244\n",
      "161  Episode Reward:  -2.4076228447692585 .Sigma: 0.4600000000000595\n",
      "162  Episode Reward:  -1112.0900802415815 .Sigma: 0.4566666666667265\n",
      "163  Episode Reward:  -1111.433098000836 .Sigma: 0.45333333333339354\n",
      "164  Episode Reward:  -1167.0745547078725 .Sigma: 0.4500000000000606\n",
      "STATIC WEIGHTS UPDATED\n",
      "165  Episode Reward:  -1087.3162013975686 .Sigma: 0.4466666666667276\n",
      "166  Episode Reward:  -1164.3418745903866 .Sigma: 0.44333333333339464\n",
      "167  Episode Reward:  -1251.4798058927506 .Sigma: 0.4400000000000617\n",
      "168  Episode Reward:  -2.009025584640821 .Sigma: 0.4366666666667287\n",
      "169  Episode Reward:  -1376.6354104872096 .Sigma: 0.43333333333339574\n",
      "170  Episode Reward:  -1114.7114481122196 .Sigma: 0.4300000000000628\n",
      "171  Episode Reward:  -1732.450077525962 .Sigma: 0.4266666666667298\n",
      "172  Episode Reward:  -1087.4164000446208 .Sigma: 0.42333333333339684\n",
      "173  Episode Reward:  -1149.2024314924818 .Sigma: 0.4200000000000639\n",
      "174  Episode Reward:  -1329.9365354586755 .Sigma: 0.4166666666667309\n",
      "175  Episode Reward:  -1303.1054517331092 .Sigma: 0.41333333333339795\n",
      "176  Episode Reward:  -1156.2986088277928 .Sigma: 0.410000000000065\n",
      "177  Episode Reward:  -1178.532508786222 .Sigma: 0.406666666666732\n",
      "178  Episode Reward:  -1159.5219601136628 .Sigma: 0.40333333333339905\n",
      "179  Episode Reward:  -1731.5794177128037 .Sigma: 0.4000000000000661\n",
      "STATIC WEIGHTS UPDATED\n",
      "180  Episode Reward:  -1731.0675954196709 .Sigma: 0.3966666666667331\n",
      "181  Episode Reward:  -2.0878561251685426 .Sigma: 0.39333333333340015\n",
      "182  Episode Reward:  -1002.1959382976489 .Sigma: 0.3900000000000672\n",
      "183  Episode Reward:  -1160.1674888497073 .Sigma: 0.3866666666667342\n",
      "184  Episode Reward:  -1311.8464567333624 .Sigma: 0.38333333333340125\n",
      "185  Episode Reward:  -1127.3653619191666 .Sigma: 0.3800000000000683\n",
      "186  Episode Reward:  -989.2946241558991 .Sigma: 0.3766666666667353\n",
      "187  Episode Reward:  -1016.0815891775127 .Sigma: 0.37333333333340235\n",
      "188  Episode Reward:  -1118.3638747267237 .Sigma: 0.3700000000000694\n",
      "189  Episode Reward:  -1005.7918373375808 .Sigma: 0.3666666666667364\n",
      "190  Episode Reward:  -1347.7741954704316 .Sigma: 0.36333333333340345\n",
      "191  Episode Reward:  -1006.2276775537872 .Sigma: 0.3600000000000705\n",
      "192  Episode Reward:  -1133.2878887201243 .Sigma: 0.3566666666667375\n",
      "193  Episode Reward:  -882.1914681385256 .Sigma: 0.35333333333340455\n",
      "194  Episode Reward:  -1239.0243328512774 .Sigma: 0.3500000000000716\n",
      "STATIC WEIGHTS UPDATED\n",
      "195  Episode Reward:  -0.8038869234733488 .Sigma: 0.3466666666667386\n",
      "196  Episode Reward:  -1184.6921200691131 .Sigma: 0.34333333333340565\n",
      "197  Episode Reward:  -988.0788575427637 .Sigma: 0.3400000000000727\n",
      "198  Episode Reward:  -1252.1336341913118 .Sigma: 0.3366666666667397\n",
      "199  Episode Reward:  -1186.9073807992884 .Sigma: 0.33333333333340676\n",
      "200  Episode Reward:  -131.2786985513544 .Sigma: 0.3300000000000738\n",
      "201  Episode Reward:  -1263.3041810544212 .Sigma: 0.3266666666667408\n",
      "202  Episode Reward:  -1.1558264962573426 .Sigma: 0.32333333333340786\n",
      "203  Episode Reward:  -1108.3176307148658 .Sigma: 0.3200000000000749\n",
      "204  Episode Reward:  -1237.6899588022613 .Sigma: 0.3166666666667419\n",
      "205  Episode Reward:  -1049.271535488666 .Sigma: 0.31333333333340896\n",
      "206  Episode Reward:  -1284.1470047033888 .Sigma: 0.310000000000076\n",
      "207  Episode Reward:  -1001.7887421856058 .Sigma: 0.306666666666743\n",
      "208  Episode Reward:  -1139.63340516349 .Sigma: 0.30333333333341006\n",
      "209  Episode Reward:  -1018.141106849214 .Sigma: 0.3000000000000771\n",
      "STATIC WEIGHTS UPDATED\n",
      "210  Episode Reward:  -989.8768539802109 .Sigma: 0.2966666666667441\n",
      "211  Episode Reward:  -1021.7405577089634 .Sigma: 0.29333333333341116\n",
      "212  Episode Reward:  -642.112241027693 .Sigma: 0.2900000000000782\n",
      "213  Episode Reward:  -992.4510860466825 .Sigma: 0.28666666666674523\n",
      "214  Episode Reward:  -641.5767893865464 .Sigma: 0.28333333333341226\n",
      "215  Episode Reward:  -126.37705314779554 .Sigma: 0.2800000000000793\n",
      "216  Episode Reward:  -1067.7048341596126 .Sigma: 0.27666666666674633\n",
      "217  Episode Reward:  -1100.8204379215458 .Sigma: 0.27333333333341336\n",
      "218  Episode Reward:  -1120.8577571768978 .Sigma: 0.2700000000000804\n",
      "219  Episode Reward:  -1141.7956702913646 .Sigma: 0.26666666666674743\n",
      "220  Episode Reward:  -960.9562895787722 .Sigma: 0.26333333333341447\n",
      "221  Episode Reward:  -1146.018563953275 .Sigma: 0.2600000000000815\n",
      "222  Episode Reward:  -1211.3056692222733 .Sigma: 0.25666666666674853\n",
      "223  Episode Reward:  -1132.2633536164174 .Sigma: 0.25333333333341557\n",
      "224  Episode Reward:  -1115.612744165127 .Sigma: 0.2500000000000826\n",
      "STATIC WEIGHTS UPDATED\n",
      "225  Episode Reward:  -1105.6220629351942 .Sigma: 0.24666666666674963\n",
      "226  Episode Reward:  -1128.57003749235 .Sigma: 0.24333333333341667\n",
      "227  Episode Reward:  -1101.1367432975014 .Sigma: 0.2400000000000837\n",
      "228  Episode Reward:  -1144.104379543151 .Sigma: 0.23666666666675074\n",
      "229  Episode Reward:  -1731.4553510537673 .Sigma: 0.23333333333341777\n",
      "230  Episode Reward:  -1299.082227664326 .Sigma: 0.2300000000000848\n",
      "231  Episode Reward:  -1085.7990194962624 .Sigma: 0.22666666666675184\n",
      "232  Episode Reward:  -1141.7370752342415 .Sigma: 0.22333333333341887\n",
      "233  Episode Reward:  -1119.684552271113 .Sigma: 0.2200000000000859\n",
      "234  Episode Reward:  -1001.8686901995195 .Sigma: 0.21666666666675294\n",
      "235  Episode Reward:  -1042.961312080059 .Sigma: 0.21333333333341997\n",
      "236  Episode Reward:  -1734.839930191402 .Sigma: 0.210000000000087\n",
      "237  Episode Reward:  -1078.4744245978395 .Sigma: 0.20666666666675404\n",
      "238  Episode Reward:  -1728.845457100572 .Sigma: 0.20333333333342107\n",
      "239  Episode Reward:  -1105.3518288271337 .Sigma: 0.2000000000000881\n",
      "STATIC WEIGHTS UPDATED\n",
      "240  Episode Reward:  -1161.320714754202 .Sigma: 0.19666666666675514\n",
      "241  Episode Reward:  -1002.3015077129335 .Sigma: 0.19333333333342217\n",
      "242  Episode Reward:  -1004.8433498909029 .Sigma: 0.1900000000000892\n",
      "243  Episode Reward:  -986.6984198233313 .Sigma: 0.18666666666675624\n",
      "244  Episode Reward:  -988.3241973809146 .Sigma: 0.18333333333342328\n",
      "245  Episode Reward:  -986.4364912976478 .Sigma: 0.1800000000000903\n",
      "246  Episode Reward:  -1012.7517330941296 .Sigma: 0.17666666666675734\n",
      "247  Episode Reward:  -1147.1818561226 .Sigma: 0.17333333333342438\n",
      "248  Episode Reward:  -1073.7113460966848 .Sigma: 0.1700000000000914\n",
      "249  Episode Reward:  -1175.2176475401773 .Sigma: 0.16666666666675845\n",
      "250  Episode Reward:  -1274.094529065063 .Sigma: 0.16333333333342548\n",
      "251  Episode Reward:  -993.3217146751523 .Sigma: 0.1600000000000925\n",
      "252  Episode Reward:  -1110.7617650453462 .Sigma: 0.15666666666675955\n",
      "253  Episode Reward:  -1074.5855681907358 .Sigma: 0.15333333333342658\n",
      "254  Episode Reward:  -1172.9920308005712 .Sigma: 0.15000000000009361\n",
      "STATIC WEIGHTS UPDATED\n",
      "255  Episode Reward:  -4.033306631751117 .Sigma: 0.14666666666676065\n",
      "256  Episode Reward:  -1038.8099511857372 .Sigma: 0.14333333333342768\n",
      "257  Episode Reward:  -875.8539929473052 .Sigma: 0.14000000000009472\n",
      "258  Episode Reward:  -0.5686924777415565 .Sigma: 0.13666666666676175\n",
      "259  Episode Reward:  -994.0540011085998 .Sigma: 0.13333333333342878\n",
      "260  Episode Reward:  -1096.7561363751827 .Sigma: 0.13000000000009582\n",
      "261  Episode Reward:  -989.6310913989009 .Sigma: 0.12666666666676285\n",
      "262  Episode Reward:  -1160.908572779392 .Sigma: 0.12333333333342988\n",
      "263  Episode Reward:  -2.9222311093629503 .Sigma: 0.12000000000009692\n",
      "264  Episode Reward:  -1143.388725953798 .Sigma: 0.11666666666676395\n",
      "265  Episode Reward:  -952.8857596279018 .Sigma: 0.11333333333343099\n",
      "266  Episode Reward:  -1012.3943082400251 .Sigma: 0.11000000000009802\n",
      "267  Episode Reward:  -1033.1197366664483 .Sigma: 0.10666666666676505\n",
      "268  Episode Reward:  -1160.8680464981217 .Sigma: 0.10333333333343209\n",
      "269  Episode Reward:  -1012.0291153708645 .Sigma: 0.10000000000009912\n",
      "STATIC WEIGHTS UPDATED\n",
      "270  Episode Reward:  -0.9765318900906265 .Sigma: 0.09666666666676615\n",
      "271  Episode Reward:  -987.3097322359945 .Sigma: 0.09333333333343319\n",
      "272  Episode Reward:  -1177.3048851771193 .Sigma: 0.09000000000010022\n",
      "273  Episode Reward:  -1260.5147593630504 .Sigma: 0.08666666666676726\n",
      "274  Episode Reward:  -3.049796802806159 .Sigma: 0.08333333333343429\n",
      "275  Episode Reward:  -1282.5590289501092 .Sigma: 0.08000000000010132\n",
      "276  Episode Reward:  -989.6120535382485 .Sigma: 0.07666666666676836\n",
      "277  Episode Reward:  -991.7314644639896 .Sigma: 0.07333333333343539\n",
      "278  Episode Reward:  -921.0210353708358 .Sigma: 0.07000000000010242\n",
      "279  Episode Reward:  -1113.496917031437 .Sigma: 0.06666666666676946\n",
      "280  Episode Reward:  -1262.5964463157568 .Sigma: 0.06333333333343649\n",
      "281  Episode Reward:  -1126.6054906980553 .Sigma: 0.060000000000103526\n",
      "282  Episode Reward:  -1020.127773016448 .Sigma: 0.05666666666677056\n",
      "283  Episode Reward:  -1115.5657026275221 .Sigma: 0.053333333333437594\n",
      "284  Episode Reward:  -990.7264782032088 .Sigma: 0.05000000000010463\n",
      "STATIC WEIGHTS UPDATED\n",
      "285  Episode Reward:  -1029.3707581378549 .Sigma: 0.04666666666677166\n",
      "286  Episode Reward:  -1060.6607234017324 .Sigma: 0.043333333333438695\n",
      "287  Episode Reward:  -1728.9259245583874 .Sigma: 0.04000000000010573\n",
      "288  Episode Reward:  -984.9806506621698 .Sigma: 0.03666666666677276\n",
      "289  Episode Reward:  -1000.8470718099521 .Sigma: 0.033333333333439796\n",
      "290  Episode Reward:  -1194.3044799675008 .Sigma: 0.03000000000010657\n",
      "291  Episode Reward:  -1144.0243842023317 .Sigma: 0.02666666666677291\n",
      "292  Episode Reward:  -1160.497049417456 .Sigma: 0.02333333333343925\n",
      "293  Episode Reward:  -902.1604558383029 .Sigma: 0.02000000000010559\n",
      "294  Episode Reward:  -989.7844805121213 .Sigma: 0.01666666666677193\n",
      "295  Episode Reward:  -1235.336128155259 .Sigma: 0.013333333333438509\n",
      "296  Episode Reward:  -969.8682205956668 .Sigma: 0.010000000000105196\n",
      "297  Episode Reward:  -910.4835314836884 .Sigma: 0.006666666666771882\n",
      "298  Episode Reward:  -993.0358591229269 .Sigma: 0.0033333333334385692\n",
      "299  Episode Reward:  -1097.314908907882 .Sigma: 1.052497119406054e-13\n",
      "STATIC WEIGHTS UPDATED\n",
      "300  Episode Reward:  -1063.5912535651303 .Sigma: -1.6666666561416956e-05\n",
      "301  Episode Reward:  -1094.4345605230428 .Sigma: -1.6666666561416956e-05\n",
      "302  Episode Reward:  -1311.1012096373101 .Sigma: -1.6666666561416956e-05\n",
      "303  Episode Reward:  -1167.2488584999528 .Sigma: -1.6666666561416956e-05\n",
      "304  Episode Reward:  -1082.8423238364774 .Sigma: -1.6666666561416956e-05\n",
      "305  Episode Reward:  -987.6047722652435 .Sigma: -1.6666666561416956e-05\n",
      "306  Episode Reward:  -998.7862724761948 .Sigma: -1.6666666561416956e-05\n",
      "307  Episode Reward:  -1007.6708028936239 .Sigma: -1.6666666561416956e-05\n",
      "308  Episode Reward:  -1090.102446593607 .Sigma: -1.6666666561416956e-05\n",
      "309  Episode Reward:  -1130.7722568559643 .Sigma: -1.6666666561416956e-05\n",
      "310  Episode Reward:  -1215.135185755756 .Sigma: -1.6666666561416956e-05\n",
      "311  Episode Reward:  -1116.6169629421306 .Sigma: -1.6666666561416956e-05\n",
      "312  Episode Reward:  -992.1086410905285 .Sigma: -1.6666666561416956e-05\n",
      "313  Episode Reward:  -980.0754587374931 .Sigma: -1.6666666561416956e-05\n",
      "314  Episode Reward:  -986.8958418685074 .Sigma: -1.6666666561416956e-05\n",
      "STATIC WEIGHTS UPDATED\n",
      "315  Episode Reward:  -1186.8531301453036 .Sigma: -1.6666666561416956e-05\n",
      "316  Episode Reward:  -1227.839204098401 .Sigma: -1.6666666561416956e-05\n",
      "317  Episode Reward:  -1113.9109580941872 .Sigma: -1.6666666561416956e-05\n",
      "318  Episode Reward:  -1265.601447385716 .Sigma: -1.6666666561416956e-05\n",
      "319  Episode Reward:  -1184.2189003965893 .Sigma: -1.6666666561416956e-05\n",
      "320  Episode Reward:  -1185.648046074558 .Sigma: -1.6666666561416956e-05\n",
      "321  Episode Reward:  -1139.291875044834 .Sigma: -1.6666666561416956e-05\n",
      "322  Episode Reward:  -976.6883713894127 .Sigma: -1.6666666561416956e-05\n",
      "323  Episode Reward:  -1211.442417146679 .Sigma: -1.6666666561416956e-05\n",
      "324  Episode Reward:  -987.9581084244113 .Sigma: -1.6666666561416956e-05\n",
      "325  Episode Reward:  -1121.2834113449387 .Sigma: -1.6666666561416956e-05\n",
      "326  Episode Reward:  -1074.4538369696845 .Sigma: -1.6666666561416956e-05\n",
      "327  Episode Reward:  -0.8208270157801885 .Sigma: -1.6666666561416956e-05\n",
      "328  Episode Reward:  -988.3865346822926 .Sigma: -1.6666666561416956e-05\n",
      "329  Episode Reward:  -977.3976451826509 .Sigma: -1.6666666561416956e-05\n",
      "STATIC WEIGHTS UPDATED\n",
      "330  Episode Reward:  -1087.3180002970819 .Sigma: -1.6666666561416956e-05\n",
      "331  Episode Reward:  -1017.4181291429393 .Sigma: -1.6666666561416956e-05\n",
      "332  Episode Reward:  -1106.500674510982 .Sigma: -1.6666666561416956e-05\n",
      "333  Episode Reward:  -814.9068768368682 .Sigma: -1.6666666561416956e-05\n",
      "334  Episode Reward:  -873.0260785614466 .Sigma: -1.6666666561416956e-05\n",
      "335  Episode Reward:  -1133.3638085886917 .Sigma: -1.6666666561416956e-05\n",
      "336  Episode Reward:  -1182.973772688075 .Sigma: -1.6666666561416956e-05\n",
      "337  Episode Reward:  -1110.420719796375 .Sigma: -1.6666666561416956e-05\n",
      "338  Episode Reward:  -817.6874300009288 .Sigma: -1.6666666561416956e-05\n",
      "339  Episode Reward:  -690.7234129639472 .Sigma: -1.6666666561416956e-05\n",
      "340  Episode Reward:  -997.9462086590876 .Sigma: -1.6666666561416956e-05\n",
      "341  Episode Reward:  -1139.0371441993514 .Sigma: -1.6666666561416956e-05\n",
      "342  Episode Reward:  -876.5845170655842 .Sigma: -1.6666666561416956e-05\n",
      "343  Episode Reward:  -989.4001914577298 .Sigma: -1.6666666561416956e-05\n",
      "344  Episode Reward:  -1010.5082225340176 .Sigma: -1.6666666561416956e-05\n",
      "STATIC WEIGHTS UPDATED\n",
      "345  Episode Reward:  -928.9720119448216 .Sigma: -1.6666666561416956e-05\n",
      "346  Episode Reward:  -1008.2419389185136 .Sigma: -1.6666666561416956e-05\n",
      "347  Episode Reward:  -875.1401344526226 .Sigma: -1.6666666561416956e-05\n",
      "348  Episode Reward:  -1003.6725663388762 .Sigma: -1.6666666561416956e-05\n",
      "349  Episode Reward:  -912.309936056351 .Sigma: -1.6666666561416956e-05\n",
      "350  Episode Reward:  -1102.162058087902 .Sigma: -1.6666666561416956e-05\n",
      "351  Episode Reward:  -879.0573084526698 .Sigma: -1.6666666561416956e-05\n",
      "352  Episode Reward:  -933.0140445870138 .Sigma: -1.6666666561416956e-05\n",
      "353  Episode Reward:  -746.9456443856494 .Sigma: -1.6666666561416956e-05\n",
      "354  Episode Reward:  -624.8534594943634 .Sigma: -1.6666666561416956e-05\n",
      "355  Episode Reward:  -378.62775394198735 .Sigma: -1.6666666561416956e-05\n",
      "356  Episode Reward:  -1074.046266797269 .Sigma: -1.6666666561416956e-05\n",
      "357  Episode Reward:  -1733.284194166705 .Sigma: -1.6666666561416956e-05\n",
      "358  Episode Reward:  -0.7911954913254396 .Sigma: -1.6666666561416956e-05\n",
      "359  Episode Reward:  -127.77343369236041 .Sigma: -1.6666666561416956e-05\n",
      "STATIC WEIGHTS UPDATED\n",
      "360  Episode Reward:  -883.0508468974043 .Sigma: -1.6666666561416956e-05\n",
      "361  Episode Reward:  -632.6667590463704 .Sigma: -1.6666666561416956e-05\n",
      "362  Episode Reward:  -807.1898192855449 .Sigma: -1.6666666561416956e-05\n",
      "363  Episode Reward:  -801.3179346264435 .Sigma: -1.6666666561416956e-05\n",
      "364  Episode Reward:  -628.0263811664598 .Sigma: -1.6666666561416956e-05\n",
      "365  Episode Reward:  -496.93444723033144 .Sigma: -1.6666666561416956e-05\n",
      "366  Episode Reward:  -1000.2061134834248 .Sigma: -1.6666666561416956e-05\n",
      "367  Episode Reward:  -502.56529968914884 .Sigma: -1.6666666561416956e-05\n",
      "368  Episode Reward:  -982.2569763107006 .Sigma: -1.6666666561416956e-05\n",
      "369  Episode Reward:  -250.04016409324268 .Sigma: -1.6666666561416956e-05\n",
      "370  Episode Reward:  -906.4022848982303 .Sigma: -1.6666666561416956e-05\n",
      "371  Episode Reward:  -500.6096563351391 .Sigma: -1.6666666561416956e-05\n",
      "372  Episode Reward:  -993.465985011478 .Sigma: -1.6666666561416956e-05\n",
      "373  Episode Reward:  -506.7862230125607 .Sigma: -1.6666666561416956e-05\n",
      "374  Episode Reward:  -840.4145809519957 .Sigma: -1.6666666561416956e-05\n",
      "STATIC WEIGHTS UPDATED\n",
      "375  Episode Reward:  -378.61765844242035 .Sigma: -1.6666666561416956e-05\n",
      "376  Episode Reward:  -755.1149609657934 .Sigma: -1.6666666561416956e-05\n",
      "377  Episode Reward:  -253.71499712658508 .Sigma: -1.6666666561416956e-05\n",
      "378  Episode Reward:  -127.56511669711946 .Sigma: -1.6666666561416956e-05\n",
      "379  Episode Reward:  -129.0432803907547 .Sigma: -1.6666666561416956e-05\n",
      "380  Episode Reward:  -249.03320014926277 .Sigma: -1.6666666561416956e-05\n",
      "381  Episode Reward:  -129.46867798023294 .Sigma: -1.6666666561416956e-05\n",
      "382  Episode Reward:  -129.34212350888782 .Sigma: -1.6666666561416956e-05\n",
      "383  Episode Reward:  -1.8840016499718475 .Sigma: -1.6666666561416956e-05\n",
      "384  Episode Reward:  -1024.9687811675485 .Sigma: -1.6666666561416956e-05\n",
      "385  Episode Reward:  -642.7586919864646 .Sigma: -1.6666666561416956e-05\n",
      "386  Episode Reward:  -252.149577320587 .Sigma: -1.6666666561416956e-05\n",
      "387  Episode Reward:  -751.0108586635168 .Sigma: -1.6666666561416956e-05\n",
      "388  Episode Reward:  -621.311810808758 .Sigma: -1.6666666561416956e-05\n",
      "389  Episode Reward:  -251.39834556743457 .Sigma: -1.6666666561416956e-05\n",
      "STATIC WEIGHTS UPDATED\n",
      "390  Episode Reward:  -957.9646768514433 .Sigma: -1.6666666561416956e-05\n",
      "391  Episode Reward:  -257.28337596373115 .Sigma: -1.6666666561416956e-05\n",
      "392  Episode Reward:  -128.835767170329 .Sigma: -1.6666666561416956e-05\n",
      "393  Episode Reward:  -745.2049717409199 .Sigma: -1.6666666561416956e-05\n",
      "394  Episode Reward:  -128.39136010234114 .Sigma: -1.6666666561416956e-05\n",
      "395  Episode Reward:  -504.5401286110998 .Sigma: -1.6666666561416956e-05\n",
      "396  Episode Reward:  -701.182714731152 .Sigma: -1.6666666561416956e-05\n",
      "397  Episode Reward:  -249.45676212486396 .Sigma: -1.6666666561416956e-05\n",
      "398  Episode Reward:  -248.7463717601436 .Sigma: -1.6666666561416956e-05\n",
      "399  Episode Reward:  -364.0983956874912 .Sigma: -1.6666666561416956e-05\n",
      "400  Episode Reward:  -1.4520837637790964 .Sigma: -1.6666666561416956e-05\n",
      "401  Episode Reward:  -0.9223539441022942 .Sigma: -1.6666666561416956e-05\n",
      "402  Episode Reward:  -122.71935208864846 .Sigma: -1.6666666561416956e-05\n",
      "403  Episode Reward:  -127.78279143339505 .Sigma: -1.6666666561416956e-05\n",
      "404  Episode Reward:  -1034.7353271933237 .Sigma: -1.6666666561416956e-05\n",
      "STATIC WEIGHTS UPDATED\n",
      "405  Episode Reward:  -124.70505372671407 .Sigma: -1.6666666561416956e-05\n",
      "406  Episode Reward:  -125.72450643959297 .Sigma: -1.6666666561416956e-05\n",
      "407  Episode Reward:  -126.4774598122803 .Sigma: -1.6666666561416956e-05\n"
     ]
    }
   ],
   "source": [
    "numberEpisodes = 1000\n",
    "discount = 0.99\n",
    "counter = 0\n",
    "\n",
    "rewards = []\n",
    "tau = 0.5\n",
    "memorySize = 100000\n",
    "memoryPreviousStates = np.zeros((memorySize,3))\n",
    "memoryStates = np.zeros((memorySize,3))\n",
    "memoryActions = np.zeros((memorySize,1))\n",
    "memoryRD = np.zeros((memorySize,2))\n",
    "numberOfLearningFromReplaySteps = 256\n",
    "itemsInMemory = 0\n",
    "initialiseReplayAfter = 1000 \n",
    "scale = 1.0\n",
    "\n",
    "for episode in range (numberEpisodes):\n",
    "\n",
    "    \n",
    "    done = False\n",
    "    prevObs = env.reset()\n",
    "    episodeReward = 0\n",
    "\n",
    "    \n",
    "    for step in range (2000):\n",
    "        if done:\n",
    "            break\n",
    "        #env.render()\n",
    "        \n",
    "\n",
    "    \n",
    "        actions = actorModel.predict(np.expand_dims(prevObs, axis = 0))[0] # Shape (2,)\n",
    "        if (scale>0.0): #Loc is the mean and scale is the standard deviation.\n",
    "            noisedActions = np.clip(actions + np.random.normal(loc=[0], scale=scale), -1, 1)\n",
    "            scale -= 1/60000 # After 300 episodes it will become 0\n",
    "        else:\n",
    "            noisedActions = actions\n",
    "        \n",
    "        obs, reward, done, _ = env.step (noisedActions)\n",
    "\n",
    "\n",
    "        memoryPreviousStates[counter] = prevObs\n",
    "        memoryStates[counter] = obs\n",
    "        memoryActions[counter] = noisedActions\n",
    "        memoryRD[counter] = [reward, float (done)]\n",
    "\n",
    "        \n",
    "        if (counter % 3000 == 0):\n",
    "            actorModel.save_weights(\"actorCriticPendulum.h5\")\n",
    "            actorModel, staticActorModel   = updateStaticWeights(actorModel, staticActorModel, tau)\n",
    "            criticModel, staticCriticModel = updateStaticWeights(criticModel, staticCriticModel, tau)\n",
    "            print (\"STATIC WEIGHTS UPDATED\")\n",
    "        \n",
    "        if (itemsInMemory < (counter+1)):\n",
    "            itemsInMemory = min ((itemsInMemory+1), memorySize)\n",
    "        if (itemsInMemory >= initialiseReplayAfter):\n",
    "            executeMemoryReplayStep(memoryPreviousStates, memoryStates, memoryActions, memoryRD, actorModel, staticActorModel, criticModel, staticCriticModel, itemsInMemory)\n",
    "\n",
    "        prevObs = obs\n",
    "        episodeReward += reward\n",
    "        counter = (counter + 1) % memorySize\n",
    "    print (episode, \" Episode Reward: \", episodeReward, \".Sigma:\", scale)\n",
    "    rewards.append(episodeReward)\n",
    "\n",
    "env.close()\n",
    "actorModel.save_weights(\"actorCriticPendulum.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXJDVmZTBKkG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NoisyPendulum.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
